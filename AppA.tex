\def\appendix{\par
\section*{APPENDIX: A}} \label{AppA}
\appendix{}

In this appendix, we prove that the partitioning of an under-determined linear system of equations (e.g. Eq.(\ref{TMEq})) into multiple sub-problems leads to increasing the $ICN$ of sub-spaces (or equivalently reducing the $CN$ of sub-spaces) (Prop.1). In addition, we prove that the variance of the error can be decreased by partitioning; and moreover, the lower-bound for the Mean Square Error (MSE) of unbiased Linear Least Square Error Estimator (LLSEE) is reduced by partitioning (Prop.2). Therefore, MDFE can provide better estimates in the least square sense. Note that, in these proofs we reasonably assume that sub-problems, generated by partitioning, are still under-determined. It is also shown that under specific conditions the partitioning scheme and the fusion process can improve the accuracy of TM estimation by improving the Signal-to-Noise Ratio (SNR).\\

% ********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
%$\bullet$ ICN Improvement
{\bf ICN Improvement:} \\

\textbf{Proposition.1:} Let $H$ be a matrix in ($ R^{m\times n}$ with rank $m$) and $H_{i}$ denotes a matrix constructed from a set of rows of $H$. Then: $ICN(H_{i})\geq ICN(H)$, or equivalently, $CN(H_{i})\leq CN(H)$.\\

$\bullet$ \textbf{Proof:}\\

The $ICN$ of $i^{th}$ local sub-matrix $H_{i}$ ($\in R^{m_{i}\times n}$ with $rank(H_{i})=m_{i}$) is defined as $ICN=\kappa^{-1}_{i}=\frac{\sigma_{min}}{\sigma_{max}}$ where $\sigma_{j}$ denotes the square root of $j^{th}$ non-zero eigenvalues of Hermitian matrix $B_{i} = H_{i}H_{i}^{T}$, and $\sigma_{min}\leq \sigma_{j} \leq \sigma_{max}$ for $j=1,...,m_{i}$.

% ($\tilde{H}$ is obtained by interchanging of rows of $H$ so that $H_{i}$ appears as the first rows of $\tilde{H}$).
Sub-matrix $B_{i}$ can be considered as the principal sub-matrix of matrix $\tilde{B}$, which can be obtained by appropriate interchanging of rows and columns of $B_{m\times m}=HH^{T}$. This can be done by operation $\tilde{B} = PBP^{T}$ where permutation matrix $P$ and $P^{T}$ are used for row and column interchange, respectively \footnote{ Here are the representation of matrices used in Prop.1, where, without loss of generality, we have assumed $H_{i}$ is constructed from $j^{th}$, $k^{th}$ and $l^{th}$ rows of $H$.  
\[
 H =
 \begin{pmatrix}
  \hdots & h_{1} & \hdots \\
  \hdots & h_{1} & \hdots \\
     &  \vdots &  \\
  \hdots & h_{m} & \hdots \\  
 \end{pmatrix}, \hspace{0.5cm}
 B = HH^{T} = 
 \begin{pmatrix}
  h_{1}h^{T}_{1} & \hdots & h_{1}h^{T}_{j} & \hdots & h_{1}h^{T}_{k} & \hdots & h_{1}h^{T}_{l} & \hdots & h_{1}h^{T}_{m}  \\
  h_{2}h^{T}_{1} & \hdots & h_{2}h^{T}_{j} & \hdots & h_{2}h^{T}_{k} & \hdots & h_{2}h^{T}_{l} & \hdots & h_{2}h^{T}_{m}  \\
                 &    &      \vdots    &    &      \vdots    &    &   \vdots       &    &    \vdots       \\                             
  h_{j}h^{T}_{1} & \hdots & h_{j}h^{T}_{j} & \hdots & h_{j}h^{T}_{k} & \hdots & h_{j}h^{T}_{l} & \hdots & h_{j}h^{T}_{m}  \\
                 &    &      \vdots    &    &      \vdots    &    &   \vdots       &    &    \vdots       \\                             
  h_{k}h^{T}_{1} & \hdots & h_{k}h^{T}_{j} & \hdots & h_{k}h^{T}_{k} & \hdots & h_{k}h^{T}_{l} & \hdots & h_{k}h^{T}_{m}  \\                 
                 &    &      \vdots    &    &      \vdots    &    &   \vdots       &    &    \vdots       \\                             
  h_{l}h^{T}_{1} & \hdots & h_{l}h^{T}_{j} & \hdots & h_{l}h^{T}_{k} & \hdots & h_{l}h^{T}_{l} & \hdots & h_{l}h^{T}_{m}  \\
                 &    &      \vdots    &    &      \vdots    &    &   \vdots       &    &    \vdots       \\                             
  h_{m}h^{T}_{1} & \hdots & h_{m}h^{T}_{j} & \hdots & h_{m}h^{T}_{k} & \hdots & h_{m}h^{T}_{l} & \hdots & h_{m}h^{T}_{m}  \\
 \end{pmatrix}
\]
\vspace{0.5cm}
\[
 H_{i} =
 \begin{pmatrix}
  \hdots & h_{j} & \hdots \\
  \hdots & h_{k} & \hdots \\
  \hdots & h_{l} & \hdots \\  
 \end{pmatrix}, \hspace{0.5cm}
 B_{i} = H_{i}H_{i}^{T} = 
 \begin{pmatrix}
  h_{j}h^{T}_{j} & h_{j}h^{T}_{k} & h_{j}h^{T}_{l}  \\
  h_{k}h^{T}_{j} & h_{k}h^{T}_{k} & h_{k}h^{T}_{l}   \\                 
  h_{l}h^{T}_{j} & h_{l}h^{T}_{k} & h_{l}h^{T}_{l}  \\
 \end{pmatrix}
\]
\vspace{0.5cm}
\[
 \tilde{B} = PBP^{T} = 
 \begin{pmatrix}
  h_{j}h^{T}_{j} & h_{j}h^{T}_{k} & h_{j}h^{T}_{l} & \hdots  \\
  h_{k}h^{T}_{j} & h_{k}h^{T}_{k} & h_{k}h^{T}_{l} & \hdots  \\
  h_{l}h^{T}_{j} & h_{l}h^{T}_{k} & h_{l}h^{T}_{l} & \hdots  \\
   \vdots        &    \vdots      &   \vdots       & \hdots  \\
 \end{pmatrix}
\]
}. This operation preserves the eigenvalues of $B$, that is, $\lambda(B)=\lambda(\tilde{B})$, which is shown below:

\begin{equation}\label{EigPres}
\begin{aligned}
det(\tilde{B}-\lambda I) = & det(PBP^{T}-\lambda I) \\
                         = & det(P(B-\lambda I)P^{T}) \hspace{0.5cm} \text{because $PP^{T}=I$ \hspace{0.5cm} \cite{Strang}} \\
                         = & det(P) det(B-\lambda I) det(P^{T}) \\
                         = & det(B-\lambda I) \hspace{0.5cm} \text{because $det(P)=det(P^{T})$ equals to $+1$ or $-1$ \hspace{0.5cm} \cite{Strang}}
\end{aligned}
\end{equation}

Therefore, using the \emph{Inclusion Principle} \footnote{Theorem\cite{Horn}: Let $A\in \mathbb{C}^n$ be a Hermitian Matrix, let $r$ be an integer with $1\leq r\leq n$ and let $A_{r}$ denote any r-by-r principal sub-matrix of $A$ (obtaining by deleting (n-r) rows and corresponding columns from A). Then for each integer $k$ such that $1 \leq k\leq r$ we have: $\lambda_{k}(A) \leq \lambda_{k}(A_{r}) \leq \lambda_{k+n-r}(A)$.} \cite{Horn}(P.189) the eigenvalues of $B_{i}$ are bounded by the eigenvalues of $B$ as:
% If any two rows or any two columns are interchanged the result is a change in sign of the determinant

\begin{equation}\label{IncPrin}
\lambda_{k}(\tilde{B})=\lambda_{k}(B)\leq \lambda_{k}(B_{i}) \leq \lambda_{k+m-m_{i}}(B)=\lambda_{k+m-m_{i}}(\tilde{B})  \hspace{1cm} k=1,...,m_{i}
\end{equation}
where eigenvalues of $B$ and $B_{i}$ are in ascending order (i.e. $\lambda{j}\leq \lambda{j+1}$). This leads to the following result:
\begin{equation}\label{IncPrin}
\kappa^{-1}_{i}:=\frac{\sigma_{1}(H_{i})}{\sigma_{m_{i}}(H_{i})}=\frac{\sqrt{\lambda_{1}(B_{i})}}{\sqrt{\lambda_{m_{i}}(B_{i})}} \geq \kappa^{-1}:=\frac{\sigma_{1}(H)}{\sigma_{m}(H)}=\frac{\sqrt{\lambda_{1}(B)}}{\sqrt{\lambda_{m}(B)}}
\end{equation}
where $\sigma_{i}$ are in ascending order for both $H$ and $H_{i}$. Therefore, the $ICN$ of sub-matricies $H_{i}$ ($i=1,...,L$) are greater than the $ICN$ of $H$. \\
See Figure \ref{fig:RedIcn14Cnfg} as an example which shows partitioning improves the $ICN$. \qed \\


% ********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
{\bf Quality Improvement using MDFE:} \\

\textbf{Proposition.2:} Let $Y=HX+\epsilon$ where $\epsilon \sim \mathcal{N}(0,\sigma^{2}_{0}I_{m})$ denotes measurement noise\footnote{Gaussian model is a valid assumption in many problems in networking, communication and signal processing (for modeling signals and, particularly, noise); for example, as it has been shown in \cite{QZhao}, measurement noise in direct TM measurements can be modeled as Gaussian noise. On the other hand, even if Gaussian is not a proper model, since this assumption simplifies the analysis, in many cases, it could help us to build a theory and provide an intuitive insight for the problem at hand which provides a road-map to solve the problem with true signal/noise model.}. Then: 1) $Var(\mathcal{E})\propto CN(H)$ where $\mathcal{E}:=X-LLSEE(X)=X-H^{\dagger}Y$, and 2) the lower-bound for the MSE of Biased LLSEE is reduced by partitioning.\\

$\bullet$ \textbf{Proof:}\\

Let $Y=HX+\epsilon$ where $H$ is an ($m\times n$) matrix with $m<n$. The LLSEE of $X$ is obtained by Eq.(\ref{PIGTMEq}) as: 

\begin{equation}\label{P2Eq1}
\begin{aligned}
\hat{X} = & (H^{T}(HH^{T})^{-1})Y=DY\\
        = & D(HX+\epsilon)\\
        = & DHX+D\epsilon\\
\end{aligned}
\end{equation}
where $D_{n\times m}(=H^{\dagger})$ is computed using the SVD of $H$ as $H=U\Sigma V^{T}$ and, accordingly, $H^{\dagger}=V\Sigma^{\dagger} U^{T}$ \footnote{Note that $U$ and $V$ are Unitary matrices, i.e. $U^{*}U=I$ and $V^{*}V=I$ where $*$ denotes Hermitian operation which is equivalent to transpose operation for real valued matrices. $U$ and $V$ are orthogonal matrices with columns forming an orthonormal basis, that is, 1) $U^{T}_{i}U_{i}=1$, $V^{T}_{i}V_{i}=1$ for all $i$s; 2) $U^{T}_{i}U_{j}=0$, $V^{T}_{i}V_{j}=0$ for all $i\neq j$ and 3) $\left\|U_{i}\right\|=1$ and $\left\|V_{i}\right\|=1$ for all $i$'s \cite{Strang, Golub}.}. Assuming $HH^{T}$ is invertible, LLSEE estimator $\hat{X} = (H^{T}(HH^{T})^{-1})Y=DY$ is a Biased estimator\footnote{This may show a point to work on un-biased estimator which improves the performance.} with Bias $B:=E[\hat{X}]-X=$ computed as:

\begin{equation}\label{P2Eq2}
\begin{aligned}
B = & E[\hat{X}]-X \\
  = & E\left[ (H^{T}(HH^{T})^{-1})Y \right] - X\\
  = & E\left[ (H^{T}(HH^{T})^{-1})(HX+\epsilon) \right] - X  \\
  = & (H^{T}(HH^{T})^{-1}H-I)E[X] \hspace{0.5in} \text{by $E[\epsilon]=0$}\\
  = & (F-I)E[X] \hspace{0.5cm} \text{where $F=DH=H^{T}(HH^{T})^{-1}H=\sum_{j=1}^{m} V_{j}V_{j}^{T}$} \\  
\end{aligned}
\end{equation}

$\bullet$ \textbf{Part 1:} \\

Now the estimation error is computed as:

\begin{equation}\label{P2Eq3}
\begin{aligned}
\mathcal{E} = & X-LLSEE(X) \\
            = & X-DY \\
            = & X-D(HX+\epsilon)\\
            = & X-DHX-D\epsilon \\
            = &(I-DH)X-D\epsilon
\end{aligned}
\end{equation}
and, the covariance matrix of error is computed using Eq.(\ref{P2Eq4}) where it is reasonably assumed that noise components are zero mean and i.i.d. with $C_{\epsilon}=Cov(\epsilon)=\sigma^{2}_{0} I_{m}$, and $\epsilon$ and $X$ are independent. 

\begin{equation}\label{P2Eq4}
\begin{aligned}
Cov(\mathcal{E}):= & E\left[ \mathcal{E} \mathcal{E}^{T}  \right]   \hspace{0.5cm} \text{(for simplicity of matrix manipulations)} \\
                 = & (I-DH) E[XX^{T}] (I-DH)^{T} \\
                 + & \underbrace{E\left[ (I-HD)X\epsilon^{T}D^{T}  \right]}_{\tiny{\text{ is zero since $\epsilon$ and $X$ are independent and $E[\epsilon]=0$ (i.e. $E[\epsilon_{i}x_{j}]=0$ for all $i=1,...,m; j=1,...,n$)}}}   \\
                 + & \underbrace{E\left[ D\epsilon X^{T}(I-HD)^{T}  \right]}_{\tiny{\text{ is zero since $\epsilon$ and $X$ are independent and $E[\epsilon]=0$ (i.e. $E[\epsilon_{i}x_{j}]=0$ for all $i=1,...,m; j=1,...,n$)}}}  \\
                 + & D E[\epsilon \epsilon^{T}] D^{T}    \\
                 = & (I-DH) C_{X} (I-DH)^{T} + \sigma^{2}_{0} DD^{T}  \\
                 = & (I-DH) C_{X} (I-DH)^{T} + \sigma^{2}_{0} \sum_{j=1}^{m} \frac{1}{\sigma^{2}_{j}} V_{j}V_{j}^{T} \hspace{0.5in} \text{ using SVD of $H$} \\
                 = & (I-F) C_{X} (I-F)^{T} + \sigma^{2}_{0} \sum_{j=1}^{m} \frac{1}{\sigma^{2}_{j}} V_{j}V_{j}^{T} \hspace{0.5in} \\
                 % = & (I-DH) C_{X} (I-DH)^{T} + \sigma^{2}_{0} (H^{T}H)^{-1}  \hspace{0.5in} \text{ see Eq.(\ref{DDT})}\\
\end{aligned}
\end{equation}
where $C_{X} = E[XX^{T}]$ and $\sigma_{i}$ represents the $i^{th}$ singular value of $H$. Therefore, $Cov(\mathcal{E})$ is proportional with $C_{X}$ (representing input structure), the structure of the matrix $H$ (coming to the picture by $V$ and $\sigma_{i}$) and the variance of noise ($\sigma_{0}^{2}$). Note that, since $F$ is not a function of singular values of $H$, thus, the first term is not a function of singular values of $H$, that is, $\{\sigma_{j}\}_{j=1}^{m}$. Since in many practical situations, where the maximum singular value $\sigma_{1}$ is bounded (that is, $\sigma_{1}\leq \xi$ (for some $\xi>0$)), the matrix is ill-conditioned because there are very small singular values, corresponding to the noise sub-space. Now, considering the fact that the diagonal elements of the second term are non-negative, hence, ill-conditioned matrices (with high CN due to small singular values) have higher noise \emph{Variance}, that is, $Var(\mathcal{E}) \propto CN(H)$ where $CN(H)=\frac{max(\Sigma)}{min(\Sigma)}=\frac{\sigma_{1}}{\sigma{m}}$ and $\sigma_{1}\geq ... \geq\sigma_{m}$. Eq.(\ref{P2Eq4}) also shows that to effectively apply MDFE framework, the structure of $X$ and $H$ must be considered.

%Now, considering the fact that the diagonal elements of both terms in $Cov(\mathcal{E})$ are positive; hence, ill-conditioned matrices (with high CN due to small singular values) have higher noise \emph{Variance}, that is, $Var(\mathcal{E}) \propto CN(H)$ where $CN(H)=\frac{max(\Sigma)}{min(\Sigma)}=\frac{\sigma_{1}}{\sigma{m}}$ and $\sigma_{1}\geq ... \geq\sigma_{m}$.


%Therefore the more ill-conditioned $H$, is the more $Cov(\mathcal{E})$ will be. On the other hand, since CN is defined as $CN(H)=\frac{max(\Sigma)}{min(\Sigma)}=\frac{\sigma_{1}}{\sigma{m}}$ and $\sigma_{1}\geq ... \geq\sigma_{m}$; thus, matrices with large condition number, increases the $Cov(\mathcal{E})$. Now, since partitioning improves the CN (Prop.1), it can reduce the $Cov(\mathcal{E})$, that is, $Cov(\mathcal{E}_{i}) \leq Cov(\mathcal{E})$ where $Cov(\mathcal{E_{i}})$ denotes the error covariance matrix of $i^{th}$ sub-space. 
%
%In many practical cases, the input $X$ and observation matrix $H$ are not under our control; therefore, reducing $\sigma^{2}_{0}$ and $CN$ could help to reduce the $Cov(\mathcal{E})$, that is, $Cov(\mathcal{E})\propto CN(H)$. In fact, what Eq.(\ref{P2Eq5}) 
%
%Therefore, $Cov(\mathcal{E})$ is proportional with $Cov(X)$. In addition, it is inversely proportional with $\sigma_{i}$. Since in many practical situations, where the maximum singular value $\sigma_{1}$ is bounded (that is, $\sigma_{1}\leq \xi$ (for some $\xi>0$)), the matrix is ill-conditioned because there are small singular values. Therefore the more ill-conditioned $H$, is the more $Cov(\mathcal{E})$ will be. On the other hand, since CN is defined as $CN(H)=\frac{max(\Sigma)}{min(\Sigma)}=\frac{\sigma_{1}}{\sigma{m}}$ and $\sigma_{1}\geq ... \geq\sigma_{m}$; thus, matrices with large condition number, increases the $Cov(\mathcal{E})$. Now, since partitioning improves the CN (Prop.1), it can reduce the $Cov(\mathcal{E})$, that is, $Cov(\mathcal{E}_{i}) \leq Cov(\mathcal{E})$ where $Cov(\mathcal{E_{i}})$ denotes the error covariance matrix of $i^{th}$ sub-space. 

%where $C_{X}=Cov(X)=E[XX^{T}]$ and $\sigma_{i}$ represents the $i^{th}$ singular value of $H$. Therefore, $Cov(\mathcal{E})$ is proportional with $Cov(X)$. In addition, it is inversely proportional with $\sigma_{i}$. Since in many practical situations, where the maximum singular value $\sigma_{1}$ is bounded (that is, $\sigma_{1}\leq \xi$ (for some $\xi>0$)), the matrix is ill-conditioned because there are small singular values. Therefore the more ill-conditioned $H$, is the more $Cov(\mathcal{E})$ will be. On the other hand, since CN is defined as $CN(H)=\frac{max(\Sigma)}{min(\Sigma)}=\frac{\sigma_{1}}{\sigma{m}}$ and $\sigma_{1}\geq ... \geq\sigma_{m}$; thus, matrices with large condition number, increases the $Cov(\mathcal{E})$. Now, since partitioning improves the CN (Prop.1), it can reduce the $Cov(\mathcal{E})$, that is, $Cov(\mathcal{E}_{i}) \leq Cov(\mathcal{E})$ where $Cov(\mathcal{E_{i}})$ denotes the error covariance matrix of $i^{th}$ sub-space. 
%

To get more insight into this analysis, let consider the simple case $C_{X}=\sigma^{2}_{X}I$. In this case, using SVD of $H$, $Cov(\mathcal{E})$ can be written as Eq.(\ref{P2Eq5}) and its Trace is computed as Eq.(\ref{P2Eq5-1}). Note that $F=F^{T}$ and $FF^{T}=F^{2}=F$; thus $F$ is a projection matrix.

\begin{equation}\label{P2Eq5}
\begin{aligned}
Cov(\mathcal{E}) = & \sigma^{2}_{X} I - \sigma^{2}_{X} \sum_{j=1}^{m} V_{j}V_{j}^{T} + \sigma^{2}_{0} \sum_{j=1}^{m} \frac{1}{\sigma^{2}_{j}} V_{j}V_{j}^{T} \\ 
%                 = & \sigma^{2}_{X} I  + \sigma^{2}_{X} \sum_{j=1}^{m} \left(\frac{1}{\sigma^{2}_{j}.SNR} -1\right) V_{j}V_{j}^{T}, \hspace{1cm} \text{$SNR=\frac{\sigma^{2}_{X}}{\sigma^{2}_{0}}$} \\
\end{aligned}
\end{equation}

\begin{equation}\label{P2Eq5-1}
\begin{aligned}
\text{Total Error Variance (TEV)} \propto & Tr\left(Cov(\mathcal{E})\right) \\
                           = & n\sigma^{2}_{X} - \sigma^{2}_{X} \sum_{l=1}^{m} \sum_{k=1}^{n} v^{2}_{kl} + \sigma^{2}_{0} \sum_{l=1}^{m} \sum_{k=1}^{n} \frac{1}{\sigma^{2}_{l}}  v^{2}_{kl} \\
 = & (n-m) \sigma^{2}_{X} + \sigma^{2}_{0} \sum_{l=1}^{m} \frac{1}{\sigma^{2}_{l}} \hspace{0.5cm} \small{\text{by construction of $V$ where $\left\|V_{l}\right\|^{2}=\sum_{k=1}^{n} v^{2}_{kl}=1$ for $l=1,...,m$}} \\
\end{aligned}
\end{equation}

Now, consider the $i^{th}$ sub-space with sub-matrix $H_{i}$ and size $m_{i}\times n_{i}$ where $m_{i}\leq m$, $n_{i}\leq n$. Since our original problem is under-determined, it is reasonable to assume that $m_{i}\leq n_{i}$. In this sub-space, $Tr\left(Cov(\mathcal{E}_{i})\right)$ can be written as:

\begin{equation}\label{P2Eq5-2}
\begin{aligned}
Tr\left(Cov(\mathcal{E}_{i})\right) = & (n_{i}-m_{i}) \sigma^{2}_{X} + \sigma^{2}_{0} \sum_{l=1}^{m_{i}} \frac{1}{\left(\sigma^{i}_{l}\right)^{2}}
\end{aligned}
\end{equation}

Therefore, for the same $\sigma^{2}_{0}$, considering the construction of $V$ in SVD (i.e. $\sum_{k=1}^{n} v^{2}_{kl}=1$ for $l=1,...,m$), $\sigma^{i}_{l}\geq \sigma_{l}$ (by Prop.1 and inclusion principle stating that $\sigma_{l} \leq \sigma_{l}^{i} \leq \sigma_{l+m-m_{i}}$), $m_{i}\leq m$ and $n_{i}\leq n$ we can conclude that:

\begin{equation}\label{P2Eq5-3}
\begin{aligned}
\sigma^{2}_{0} \sum_{l=1}^{m_{i}} \frac{1}{\left(\sigma^{i}_{l}\right)^{2}} \leq  \sigma^{2}_{0} \sum_{l=1}^{m} \frac{1}{\sigma^{2}_{l}}
\end{aligned}
\end{equation}

Therefore, in the worst case, to reduce the Total Error Variance (TEV) by partitioning, we need to have the following condition for all sub-spaces. 

\begin{equation}\label{P2Eq5-4}
\begin{aligned}
(n_{i}-m_{i}) \leq (n-m) \hspace{0.5 cm} \text{for all $i=1,...,L$}
\end{aligned}
\end{equation}

Achieving this condition is not hard for fat and low-density matrices, where $n>m>m_{i}$ \footnote{This can be used for designing L and, possibly, sub-spaces where we can jointly maximize the $CN$ and $RoD=\frac{m_{i}}{n_{i}}$ of sub-spaces. This equation can also be used to introduce the following rule of thumb for designing $L$ and subspaces in the worst case:
\begin{equation}\label{P2Eq5-5}
\begin{aligned}
 & \sum_{i=1}^{L} \alpha_{i}(n_{i}-m_{i}) \leq (n-m) \hspace{0.5 cm} \hspace{0.5 cm} \text{for $\alpha_{i}\geq 1$} \\
 & \text{subject to: partition $P$ is formed and all $\{x_{i}\}_{i=1}^{n}$ are observed.}
\end{aligned}
\end{equation}
This may be achieved by the proper distribution of rows with highest number of unknowns among other subsets (formed by Alg.\ref{alg:GrdICNAlg}), for example.}. This result is compatible with our results in Table \ref{tab:SSChar} where $n-m=182-50=132$, $n_{1}-m_{1}=70-7=63$, $n_{2}-m_{2}=32-4=28$, $n_{3}-m_{3}=43-8=35$, $n_{4}-m_{4}=99-18=81$ and $n_{5}-m_{5}=69-13=56$, and, the best and worst performances obtained in sub-spaces $2$ and $4$, respectively (see Figure \ref{fig:L2ErrPrfQRP}). This result is also match with our investigation on random matrices in Section \ref{sec:RandomObsMtx} where it is shown that the performance for dense matrices degrades because condition Eq.(\ref{P2Eq5-4}) con be violated with higher probability. \qed \\

%which, similar to Eq.(\ref{P2Eq4}), shows that $Cov(\mathcal{E})$ is a function of $C_{X}$, the structure of the matrix $H$ (coming to the picture by $V$ and $\sigma_{i}$) and the variance of noise ($\sigma_{0}^{2}$). In many practical cases, the input $X$ and observation matrix $H$ are not under our control; therefore, reducing $\sigma^{2}_{0}$ and $CN$ could help to reduce the $Cov(\mathcal{E})$, that is, $Cov(\mathcal{E})\propto CN(H)$. In fact, what Eq.(\ref{P2Eq5}) 

%************ rank-1 approximation ************
%*************** trace ************
%Note that, here we have assumed that the dominant factor in Eq.(\ref{P2Eq5}) 

$\bullet$ \textbf{Part 2:} \\

The covariance matrix $C_{\hat{X}}$ for Biased estimator $\hat{X}=(H^{T}(HH^{T})^{-1})Y=DY$ is defined as  $C_{\hat{X}} = E\{[\hat{X}-E[\hat{X}]] [\hat{X}-E[\hat{X}]]^{*} \}$. This covariance matrix must satisfy Eq.(\ref{P2Eq6}) \cite{Hero, Eldar}:

\begin{equation}\label{P2Eq6}
C_{\hat{X}} \succeq (I+D_{B})J^{-1}(I+D_{B})^{*} := C(D_{B})
\end{equation}
where $J$ is the Fisher information matrix (which is assumed to be non-singular) and $D_{B}$ is the bias gradient matrix defined as:

\begin{equation}\label{P2Eq7}
D_{B} =  \frac{\partial B}{\partial X} \hspace{0.5cm}  \text{where for LLSEE, $D_{B}$ is}  \hspace{1cm} D_{B}=(F-I)\frac{\partial E[X]}{X}
\end{equation}

To simplify the analysis, let assume $\frac{\partial E[X]}{X}=I$ (where $\frac{\partial E[X]}{X}= \left[ \frac{\partial E[x_{i}]}{x_{j}} \right]$ for $i,j=1,...,n$) and consider linear Gaussian model $Y=HX+\epsilon$ where $\epsilon \sim \mathcal{N}(0,\sigma^{2}_{0}I_{m})$ and $\{\epsilon_{i}\}_{i=1}^{m}$ are i.i.d.; then:

\begin{equation}\label{P2Eq8}
\begin{aligned}
p(Y;X)    = & \frac{1}{(2\pi\sigma^{2}_{0})^{\frac{m}{2}}} e^{-\frac{1}{2\sigma^{2}{0}} (Y-HX)^{T}(Y-HX)} \\
Ln p(Y;X) = & -(2\pi\sigma^{2}_{0})^{\frac{m}{2}} - \frac{1}{2\sigma^{2}{0}} (Y-HX)^{T}(Y-HX)
\end{aligned}
\end{equation}
and, accordingly, Fisher information matrix $J$ \cite{Kay} is computed using $J_{ij}=\left\{-E\left[ \frac{\partial^{2} LnP(Y;X)}{\partial x_{i} \partial x_{j}}  \right]\right\}_{i,j=1}^{n}$ as:

%\footnote{See \cite{Kay} and the fact that:
%\begin{equation}\label{P2Eq8-1}
%\begin{aligned}
%\frac{\partial Ln p(Y;X)}{\partial X} = & Y^{T}H-X^{T}(H^{T}H) \\
%                                      = & Y^{T}IH-X^{T}(H^{T}H) \\
%                                      = & Y^{T}(HH^{T})^{-1}(HH^{T})H-X^{T}(H^{T}H) \\
%                                      = & \left[Y^{T}(HH^{T})^{-1}H-X^{T}\right](H^{T}H) \\
%                                      = & \left[Y^{T}(HH^{T})^{-1}H-X^{T}\right]J                                    
%\end{aligned}
%\end{equation}
%}

\begin{equation}\label{P2Eq9}
J  = \frac{1}{\sigma^{2}_{0}} H^{T}H \Rightarrow J^{-1}=\sigma^{2}_{0} (H^{T}H)^{-1}
\end{equation}

For LLSEE $\hat{X}=(H^{T}(HH^{T})^{-1})Y$, $C_{\hat{X}}$,  is bounded using Eq.(\ref{P2Eq6}) as:

\begin{equation}\label{P2Eq10}
\begin{aligned}
C_{\hat{X}} & \succeq  (I+D_{B})J^{-1}(I+D_{B})^{*}=C(D_{B}) \\
            & \succeq  \sigma^{2}_{0} F (H^{T}H)^{-1} F \\
            & \succeq  \sigma^{2}_{0} \left( \sum_{j=1}^{m} V_{j}V_{j}^{T} \right)  \left( \sum_{j=1}^{m} \frac{1}{\sigma^{2}_{j}} V_{j}V_{j}^{T}  \right)   \left( \sum_{j=1}^{m} V_{j}V_{j}^{T} \right) \\
            & \succeq  \sigma^{2}_{0}   \left( \sum_{j=1}^{m} \frac{1}{\sigma^{2}_{j}} V_{j}V_{j}^{T} \right) \hspace{0.5cm} \text{by $V^{T}V=I$}\\
            % & \succeq  \sigma^{2}_{0} \sum_{l=1}^{m} \sum_{k=1}^{n} \frac{1}{\sigma^{2}_{l}}  v^{2}_{kl}
\end{aligned}
\end{equation}
and, accordingly, $Tr(C_{\hat{X}})=\sum_{i=1}^{n}E\left[\hat{x}_{i}-E[\hat{x_{i}}]\right]^{2}$ is bounded by:

\begin{equation}\label{P2Eq11}
\begin{aligned}
Tr(C_{\hat{X}}) & \geq  \sigma^{2}_{0} \sum_{l=1}^{m} \frac{1}{\sigma^{2}_{l}}
\end{aligned}
\end{equation}

Similarly, the $Tr(C_{\hat{X}_{i}})$ for $i^{th}$ sub-space is bounded as:  

\begin{equation}\label{P2Eq11-1}
\begin{aligned}
Tr(C_{\hat{X}_{i}}) & \geq  \sigma^{2}_{0} \sum_{l=1}^{m_{i}} \frac{1}{(\sigma^{i}_{l})^{2}}
\end{aligned}
\end{equation}

According to Eq.(\ref{P2Eq5-3}) and with the same argument, it is clear that: $\sigma^{2}_{0} \sum_{l=1}^{m_{i}} \frac{1}{(\sigma^{i}_{l})^{2}} \leq \sigma^{2}_{0} \sum_{l=1}^{m} \frac{1}{\sigma^{2}_{l}}$. Therefore, the lower-bound for the MSE of biased LLSEE is reduced by partitioning. That is, LLSEE is \emph{potentially} able to obtain lower error in sub-spaces.

\qed\\

% ********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

%$\bullet$ SNR Improvement
{\bf SNR Improvement:}\\

\textbf{Proposition.3:} Partitioning can enhance the Improvement Factor (IF) of each sub-space which is defined as the $IF_{i}=\frac{SNR^{G}}{SNR^{L}_{i}}$ (for $i=1, ... ,L$) where $SNR:=\frac{E[x^{2}_{j}]}{\text{noise power at the output of the estimator}}$ (Note that; the less the IF is, the more SNR improvement using MDFE is achieved).\\

$\bullet$ \textbf{Discussion} \\

Consider Eq.(\ref{PIGTMEq}) that provides the LSE solution for the global TM estimation problem (Eq.(\ref{TMEq})); in this equation $H$ is an $m\times n$ matrix and $H^{\dagger}$ is an $n\times m$ matrix. Assume, $\hat{x}^{G}_{i}$ is modeled as $\hat{x}^{G}_{i}=x^{G}_{i}+n^{G}_{i}$ where $n^{G}_{i}$ is normally distributed as $n^{G}_{i} \sim \mathcal{N}(0,\sigma^{2}_{G})$  ($\sigma^{2}_{G}$ is the variance of $n^{G}_{i}$). Since $\hat{x}^{G}_{i}$ is linearly related to link counts $y_{i}$s and OD flows $x_{i}$s, thus, the distribution of $n^{G}_{i}$ can be represented as:
\begin{equation}\label{App1}
n^{G}_{i}\sim \mathcal{N}(0,\sigma^{2}_{G})=\mathcal{N}(0,\mathcal{K}^{G}\sigma^{2}_{y})=\mathcal{N}(0,\mathcal{K}^{G}N^{G}\sigma^{2}_{x})
\end{equation}
where $\mathcal{K}^{G}:=\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{m} H^{\dagger^{2}}(i,j)$ represents the average energy of the rows of $H^{\dagger}$ and the average number of the 1's of rows of $H$ is $N^{G}:=\frac{1}{m}\sum_{i=1}^{m}\sum_{j=1}^{n} H(i,j)$. Therefore, the average SNR of $\hat{x}^{G}_{i}$ is expressed by Eq.(\ref{App2}). This models, also, confirms that assuming Normal distribution for OD flows $\{x_{i}\}_{i=1}^{n}$ \cite{Medina,Cao}, then link counts $\{y_{i}\}_{i=1}^{m}$s and $\{\hat{x}^{G}_{i}\}_{i=1}^{n}$ follow Normal distributions.
\begin{equation}\label{App2}
SNR^{G} = \frac{E[x^{2}_{i}]}{\mathcal{K}^{G}N^{G}\sigma^{2}_{x}}
\end{equation}

Let partitions this problem into $L$ sub-problems as in Section \ref{sec:SecTMEMDF}. Consider $x^{l}_{i}$ as the estimation of $i^{th}$ OD flow from $l^{th}$ sub-space, and assume that $\hat{x}^{l}_{i} = x^{l}_{i}+n^{l}_{i}$ where $n^{l}_{i} \sim \mathcal{N}(0,\sigma^{2}_{l})$ ($\sigma^{2}_{l}$ is the variance of $n^{l}_{i}$). By the same argument, the distribution of $n^{l}_{i}$ can be represented as:
\begin{equation}\label{App3}
n^{l}_{i}\sim \mathcal{N}(0,\sigma^{2}_{l})=\mathcal{N}(0,\mathcal{K}^{l}\sigma^{2}_{y})=\mathcal{N}(0,\mathcal{K}^{l}N^{l}\sigma^{2}_{x})
\end{equation}
where $\mathcal{K}^{l}:=\frac{1}{|J_{l}|}\sum_{i=1}^{|J_{l}|}\sum_{j=1}^{|I_{l}|} H^{\dagger^{2}}_{l}(i,j)$ represents the average energy of the rows of $H^{\dagger}_{l}$ and the average number of the 1's of rows of $H_{l}$ is $N^{l}:=\frac{1}{|I_{l}|}\sum_{i=1}^{|I_{l}|}\sum_{j=1}^{|J_{l}|} H(i,j)$. By considering different $N^{l}$ for each sub-space a more worse case scenario is considered when MD-TME is compared with the global TM estimation. Accordingly, the the average SNR of $\hat{x}^{l}_{i}$ is:
\begin{equation}\label{App4}
SNR^{l} = \frac{E[x^{2}_{i}]}{\mathcal{K}^{l}N^{l}\sigma^{2}_{x}}
\end{equation}

Thus, if $SNR^{l}>SNR^{G}$, it is expected to have a better estimate in $l^{th}$ sub-space; this condition can be expressed as:
\begin{equation}\label{App5}
SNR^{l}>SNR^{G} \Leftrightarrow IF:= \frac{\mathcal{K}^{l}N^{l}}{\mathcal{K}^{G}N^{G}}<1
\end{equation}
where $IF$ denotes the Improvement Factor. Note that the less the $IF$ is, the more the SNR improvement is. If $x_{i}$ is observed in $\eta$ sub-spaces, then the variance of the output of the averaging fusion process (that is, $x^{Avg}_{i}=\frac{1}{\eta} \sum_{j=1}^{\eta} x_{i}^{j}$) is:
\begin{equation}\label{App6}
Var\left(x^{Avg}_{i}\right) = \frac{\sum_{l=1}^{L} \mathcal{K}^{l}N^{l}}{\eta^{2}} \sigma^{2}_{x}
\end{equation}

In this case, SNR improvement is achieved if:
\begin{equation}\label{App7}
SNR^{Avg}>SNR^{G} \Leftrightarrow IF^{Avg}:= \frac{1}{\eta^{2}}\frac{\sum_{l=1}^{L} \mathcal{K}^{l}N^{l}}{\mathcal{K}^{G}N^{G}}<1
\end{equation}
where $IF^{Avg}$ denotes the Improvement Factor with Averaging. Therefore, by satisfying conditions \ref{App5} and \ref{App7}, MD-TME is expected to have improvement in the SNR of the TM estimation problem.

Assuming $\mathcal{K}^{l}$s and $N^{l}$ are equal for $l=1,...,L$, and $N^{G}=N^{l}$, and also assuming each OD flow is observed in $\eta=\left\lceil \frac{L}{2}+1\right\rceil$ sub-spaces (in average), then, the condition above is expressed as:
\begin{equation}\label{App8}
SNR^{MD-TME}>SNR^{G} \Leftrightarrow IF0:= \frac{1}{\left\lceil \frac{L}{2}+1\right\rceil}\frac{\mathcal{K}^{l}}{\mathcal{K}^{G}}<1
\end{equation}
where $IF0$ denotes a simple role of thumb to justify the improvement factor. \qed\\
